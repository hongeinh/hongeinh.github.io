---
layout: default
title: "RAG and MCP"
date: 2025-04-02
---
# Introduction to Retrieval-Augmented Generation (RAG) and Model Context Protocol (MCP)

## Foundational skills
* NLP basics: embeddings (BERT), tokenization, and transformers.
* Retrieval systems: vector search (eg, cosine similarity) and databases like FAISS and Pinecone
* Python: libraries like `transformers`, `langchain` and `faiss`
* Context management: how LLMs handle context windows and memory (sliding windows, summarization)

Resources:
[NLP course](https://huggingface.co/learn/nlp-course/en/chapter0/1?fw=pt)
## Retrieval-Augmented Generation (RAG)
RAG is a hybrid AI approach that combines retrieval of external data with generative language models to produce more accurate, context-rich responses.   
[Vector database](https://towardsdatascience.com/deep-dive-into-vector-databases-by-hand-e9ab71f54f80/)
#### How it works
1. **Retrieval**: The model retrieves relevant information from a knowledge base or external sources.
2. **Generation**: The model generates a response based on the retrieved information.

#### Why is it useful:
1. **Contextual Understanding**: RAG allows models to understand the context of a question or prompt better.
2. **Accuracy**: By incorporating external information, RAG can provide more accurate responses.
3. **Versatility**: RAG can be applied to various domains, including customer support, knowledge bases, and more.

=> It overcomes LLM limitations like outdated knowledge or lack of specificity by grounding responses in real-time or domain-specific data.  
=> Use cases: Chatbots, research assistants, customer support, and more.

### Key components
1. **Retriever**: 
* Use embeddings to convert text into vectors
* Search a database (FAISS) for relevant matches
* Tools: Sentence-Transformers, DPR (Dense Passage Retrieval)

2. **Generator**:
* An LLM (LLaMA, GPT) that takes retrieved data as input
* Focus: Prompt engineering to integrate context smoothly
* Tools: GPT, LLaMA, T5, BLOOM

3. **Integration**:
* Combine retriever and generator


## Model Context Protocol (MCP)
MCP is a protocol that defines the communication between a model and a knowledge base. It allows the model to retrieve relevant information from the knowledge base and use it to generate a response.  
=> Think of it as a way to make RAG smarter and more autonomous (e.g., in agentic RAG)

#### Hypothesized Role:
1. **Knowledge Retrieval**: MCP enables the model to retrieve relevant information from a knowledge base.
2. **Contextual Understanding**: MCP allows the model to understand the context of a question or prompt better.
3. **Versatility**: MCP can be applied to various domains, including customer support, knowledge bases, and more.

MCP could define rules or mechanisms for:
* Tracking conversation history to refine retrieval.
* Deciding what context to prioritize or discard.
* Enabling iterative or multi-step reasoning (e.g., asking follow-ups).

#### What to study
* **Context in RAG**: How current RAG systems handle context (e.g., fixed-size windows, retrieved docs as prompts).
* **Agentic RAG**: Systems that adapt retrieval dynamically (e.g., “True Agentic RAG” on Medium, Jan 2025). MCP might formalize this.
* **Iterative Retrieval**: Protocols for refining context over multiple steps (e.g., asking clarifying questions).